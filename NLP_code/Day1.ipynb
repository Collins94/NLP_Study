{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "띄어쓰기로 구분된 문장:  ['옥련아,', '총에', '맞아', '죽었느냐,', '창에', '찔려', '죽었느냐,', '사람에게', '밟혀', '죽었느냐'] \n",
      "\n",
      "형태소 별로 구분된 문장:  ['옥련', '아', ',', '총', '에', '맞아', '죽었느냐', ',', '창', '에', '찔려', '죽었느냐', ',', '사람', '에게', '밟혀', '죽었느냐'] \n",
      "\n",
      "형태소와 그에 따른 품사로 분류된 문장:  [('옥련', 'Noun'), ('아', 'Josa'), (',', 'Punctuation'), ('총', 'Noun'), ('에', 'Josa'), ('맞아', 'Verb'), ('죽었느냐', 'Verb'), (',', 'Punctuation'), ('창', 'Noun'), ('에', 'Josa'), ('찔려', 'Verb'), ('죽었느냐', 'Verb'), (',', 'Punctuation'), ('사람', 'Noun'), ('에게', 'Josa'), ('밟혀', 'Verb'), ('죽었느냐', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Okt \n",
    "\n",
    "DATA_PATH = \"./data/blood_rain.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    # 그냥 open보다 with open이 메모리 사용 적음 인코딩에러가 있어 utf-8로 다시 인코딩해주었습니다.\n",
    "    with open(path, 'r', encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "def doc2para(writing):\n",
    "    paragraphs = []\n",
    "    # 우선 \\n기준으로 다잘라서 분리해줍니다.\n",
    "    splited = writing.split('\\n')\n",
    "    # 글을 합치기 위한 빈칸 변수선언\n",
    "    para=\"\"\n",
    "    \n",
    "    for token in splited:\n",
    "        try:\n",
    "            # 분리된 토큰들의 마지막글자가 .이 아닐경우 토큰들을 다 더해줍니다.\n",
    "            if token[-1] != '.':\n",
    "                para+= token\n",
    "            # 마지막 글자가 .이 나오는 경우 .이 나온 토큰까지 더하고나서 리스트에 추가합니다. 그후 para변수를 다시 초기화시킵니다.\n",
    "            else:\n",
    "                para+=token\n",
    "                paragraphs.append(para)\n",
    "                para =\"\"\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def para2sen(paragraph):\n",
    "    sentences = []\n",
    "    # 문단을 문장으로 .을 기준으로 분리시킵니다.\n",
    "    sentences = paragraph.split('.')\n",
    "    \n",
    "    # sentences 내 문장들에 대해서 \"?\"로 재분할 한 후, ndarray.flatten()을 활용하여 재분할된 문장이 합쳐질 수 있도록 리스트로 만들어 줍니다. (\"!\"에 대해서도 마찬가지로 적용합니다.) \n",
    "    # ndarray.flatten -> Return a copy of the array collapsed into one dimension.\n",
    "    sentences = np.array([sen.split(\"?\") for sen in sentences])\n",
    "    sentences = list(sentences.flatten())\n",
    "    sentences = np.array([sen.split(\"!\") for sen in sentences])\n",
    "    sentences = list(sentences.flatten())\n",
    "    sentences = [ sentence.replace('\"','') for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "# 문장을 공백제거후 빈칸 기준으로 분리해줍니다.\n",
    "def sen2words_byspace(sentence):\n",
    "    words = []\n",
    "    words = sentence.strip().split(\" \")\n",
    "    return words\n",
    "\n",
    "# 위에서한 노가다를 4문장으로 압축했습니다. Konlpy Okt를 이용해서요 (사실은 4줄짜리 코드였답니다...!)\n",
    "def sen2morph(sentence):\n",
    "    morphs = []\n",
    "    analyzer = Okt()\n",
    "    morphs = analyzer.morphs(sentence)\n",
    "    return morphs\n",
    "\n",
    "# 형태소와 품사를 튜플형태로 나타내는 것은 pos 그냥 형태소만 나타내려면 morphs\n",
    "def analyzing_morphs(sentence):\n",
    "    twitter= Okt()\n",
    "    return twitter.pos(sentence)\n",
    "    \n",
    "## 위에서 정의한 함수들을 바탕으로 문서를 토큰화를 진행합니다.\n",
    "def main():\n",
    "    blood_rain = load_data(DATA_PATH)\n",
    "    paragraphs = doc2para(blood_rain)\n",
    "    sentences = para2sen(paragraphs[4])\n",
    "    words_byspace = sen2words_byspace(sentences[3])\n",
    "    words_bymorphs = sen2morph(sentences[3])\n",
    "    morphs_analyzed = analyzing_morphs(sentences[3])\n",
    "    \n",
    "    # 출력을 통해 토큰화가 잘 되었는지 확인합니다.\n",
    "    print(\"띄어쓰기로 구분된 문장: \", words_byspace,'\\n')\n",
    "    print(\"형태소 별로 구분된 문장: \", words_bymorphs,'\\n')\n",
    "    print(\"형태소와 그에 따른 품사로 분류된 문장: \", morphs_analyzed)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형법과 군형법 문서의 TF-IDF값은  0.8906327539456029  입니다.\n",
      "형법과 특허법 문서의 TF-IDF값은  0.7899236059911972  입니다.\n"
     ]
    }
   ],
   "source": [
    "import response\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "PATH_1 = \"./data/criminal_law.txt\"\n",
    "PATH_2 = \"./data/military_criminal_law.txt\"\n",
    "PATH_3 = \"./data/patent_law.txt\"\n",
    "\n",
    "# 코사인 유사도를 구하는 함수입니다.\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    return np.dot(x,y) / (np.linalg.norm(x)*np.linalg.norm(y))\n",
    "\n",
    "# TODO: BoW를 딕셔너리 형태로 출력하는 함수를 만들어 봅니다.\n",
    "def bag_of_words(tokenized_sentences):\n",
    "    word_dict={}\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for token in tokenized_sentence:\n",
    "            # TODO: 기존 word_dict 에 그 값이 있으면 1을 더해주고, 없으면 1을 부여합니다.\n",
    "            if not token in word_dict:\n",
    "                word_dict[token] = 1\n",
    "            elif token in word_dict:\n",
    "                word_dict[token] +=1 \n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    file=open(path, 'r', encoding = 'utf-8')\n",
    "    output=str(file.read())\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_splited_doc(path):\n",
    "    text = read_txt(path)\n",
    "    analyzer = Okt()\n",
    "    output = analyzer.morphs(text)\n",
    "    return output\n",
    "\n",
    "\n",
    "# TODO: 한 문서 내에서 한 단어의 등장 횟수를 출력하는 함수를 만들어 봅니다. \n",
    "def tf(doc, word):\n",
    "    return doc.count(word)\n",
    "    \n",
    "\n",
    "# 여러 문서 내에서 한 단어의 등장 횟수를 출력하는 함수입니다.. \n",
    "def idf(docs, word):\n",
    "    num=0\n",
    "    for doc in docs:\n",
    "        if doc.count(word)>0:\n",
    "            num+=1\n",
    "    return np.log(len(docs)/(1+num))\n",
    "\n",
    "\n",
    "# TODO: 함수 tf와 idf를 활용하여 tf_idf 함수를 만들어 봅니다. \n",
    "def tf_idf(docs, bow):\n",
    "    len_vector= len(bow)\n",
    "    vectors=[]\n",
    "    keys = list(bow.keys())\n",
    "    for doc in docs:\n",
    "        vector = []\n",
    "        for i,key in enumerate(keys):\n",
    "            #TODO: TF와 IDF값을 곱하여 준 후, 변수 vector에 추가하여 줍니다.\n",
    "            vector.append(tf(doc,key)*idf(docs,key))\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return vectors\n",
    "    \n",
    "    \n",
    "def main():\n",
    "\n",
    "    criminal_law = get_splited_doc(PATH_1)\n",
    "    military_criminal_law = get_splited_doc(PATH_2)\n",
    "    patent_law = get_splited_doc(PATH_3)\n",
    "\n",
    "    total = [criminal_law, military_criminal_law, patent_law]\n",
    "\n",
    "    bow = bag_of_words(total)\n",
    "\n",
    "    vecs_tfIdf = tf_idf(total, bow)\n",
    "    \n",
    "    criminal, military_criminal, patent = vecs_tfIdf\n",
    "\n",
    "    sml_c_m = cosine_similarity(criminal, military_criminal)\n",
    "    sml_c_p = cosine_similarity(criminal, patent)\n",
    "    \n",
    "    print(\"형법과 군형법 문서의 TF-IDF값은 \", sml_c_m, ' 입니다.')\n",
    "    print(\"형법과 특허법 문서의 TF-IDF값은 \", sml_c_p, ' 입니다.')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문장과 두 번째 문장의 자카트 유사도는 0.444444 입니다.\n",
      "첫 번째 문장과 세 번째 문장의 자카트 유사도는 0.000000 입니다.\n",
      "두 번째 문장과 세 번째 문장의 자카트 유사도는 0.083333 입니다.\n",
      "<전체 문장에 대한 BoW>\n",
      " {'나': 2, '는': 2, '어제': 2, '잠': 1, '을': 2, '못': 1, '잤습니다': 1, '밥': 3, '굶었습니다': 1, '오늘': 2, '은': 2, '맛': 1, '이': 1, '없습니다': 1, '.': 2, '별로': 1, '입니다': 1}\n",
      "첫 번째 문장의 벡터: [1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "두 번째 문장의 벡터: [1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "세 번째 문장의 벡터: [1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "첫 번째 문장과 두 번째 문장의 코사인 유사도는 0.617213 입니다.\n",
      "첫 번째 문장과 세 번째 문장의 코사인 유사도는 0.000000 입니다.\n"
     ]
    }
   ],
   "source": [
    "import response\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# (다음 장에서 살펴볼) 원-핫 인코딩을 구현하는 함수입니다.\n",
    "def one_hot(x, depth):\n",
    "    output = []\n",
    "    for i in range(depth):\n",
    "        if i == x:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def sum_list(X, depth):\n",
    "    output = np.zeros(depth)\n",
    "    for _list in X:\n",
    "        output = np.add(output, _list)\n",
    "    return output\n",
    "\n",
    "\n",
    "# TODO: BoW를 출력하는 함수를 만들어 봅니다.\n",
    "def bag_of_words(tokenized_sentences):\n",
    "    word_dict = {}\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for token in tokenized_sentence:\n",
    "            # 구현 방법은 지난 장에서와 동일합니다.\n",
    "            if token not in word_dict.keys():\n",
    "                word_dict[token] = 1\n",
    "            else:\n",
    "                word_dict[token] +=1\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "# TODO:자카드 유사도(Jaccard Similarity)를 구하는 함수를 만들어 봅니다.\n",
    "def jaccard(X, Y):\n",
    "    # TODO: 요소의 중복을 허용하는 리스트의 길이를 len_total에 저장합니다.\n",
    "    len_total = len(X+Y)\n",
    "    # TODO: 요소의 중복을 허용하지 않는 리스트(두 리스트의 합집합)의 길이를 len_union에 저장합니다.\n",
    "    len_union = len(list(set(X+Y)))\n",
    "    # TODO: len_total과 len_union을 이용하여 중복된 리스트(두 리스트의 교집합)의 길이를 len_inter 에 저장합\n",
    "    len_inter = len_total- len_union\n",
    "                    \n",
    "    return len_inter / len_union\n",
    "\n",
    "\n",
    "def cosine(x, y):\n",
    "    # TODO: Numpy를 이용하여(np.dot(), np.linalg.norm()) 코사인 유사도를 계산합니다.\n",
    "    return np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "\n",
    "def main():\n",
    "    sentences = [\"나는 어제 잠을 못 잤습니다\", \"나는 어제 밥을 굶었습니다\", \"오늘 밥은 맛이 없습니다.\", \"오늘 밥은 별로입니다.\"]\n",
    "    analyzer = Okt()\n",
    "    tokenized = [ analyzer.morphs(sen) for sen in sentences ]\n",
    "\n",
    "    ## 토큰화된 문장을 바탕으로 자카드 유사도를 출력해 봅니다.\n",
    "    print('첫 번째 문장과 두 번째 문장의 자카트 유사도는 %f 입니다.' %(jaccard(tokenized[0], tokenized[1])))\n",
    "    print('첫 번째 문장과 세 번째 문장의 자카트 유사도는 %f 입니다.' % (jaccard(tokenized[0], tokenized[2])))\n",
    "    print('두 번째 문장과 세 번째 문장의 자카트 유사도는 %f 입니다.' % (jaccard(tokenized[1], tokenized[2])))\n",
    "    ## 토큰화된 문장을 바탕으로 BoW를 만듭니다.\n",
    "\n",
    "    BoW = bag_of_words(tokenized)\n",
    "    print(\"<전체 문장에 대한 BoW>\\n\",BoW)\n",
    "\n",
    "    ## {'word':index} 딕셔너리를 만듭니다.\n",
    "    word_index = { k:v for v,k in enumerate(BoW.keys())}\n",
    "\n",
    "    ## word_index_dict의 길이를 구하고 이를 바탕으로 원-핫 인코딩을 진행합니다.\n",
    "    len_wordIndex = len(word_index)\n",
    "\n",
    "    sen1 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[0]]\n",
    "    sen2 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[1]]\n",
    "    sen3 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[2]]\n",
    "\n",
    "    ## 각각의 단어별 임베딩된 것을 더하여 문장의 임베딩 값으로 나타냅니다.\n",
    "    sen1_onehot = sum_list(sen1, len_wordIndex)\n",
    "    sen2_onehot = sum_list(sen2, len_wordIndex)\n",
    "    sen3_onehot = sum_list(sen3, len_wordIndex)\n",
    "\n",
    "    print(\"첫 번째 문장의 벡터:\",sen1_onehot)\n",
    "    print(\"두 번째 문장의 벡터:\", sen1_onehot)\n",
    "    print(\"세 번째 문장의 벡터:\", sen1_onehot)\n",
    "\n",
    "    ## 이를 바탕으로 코사인 유사도를 출력합니다.\n",
    "    sm_12 = cosine(sen1_onehot, sen2_onehot)\n",
    "    sm_13 = cosine(sen1_onehot, sen3_onehot)\n",
    "\n",
    "    print(\"첫 번째 문장과 두 번째 문장의 코사인 유사도는 %f 입니다.\" %sm_12)\n",
    "    print(\"첫 번째 문장과 세 번째 문장의 코사인 유사도는 %f 입니다.\" %sm_13)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
